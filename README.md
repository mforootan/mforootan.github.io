# <img src="image/4mobile_green.jpg" alt="Description of the image" height="50"> massiHUB
### Data Analytics Portfolio
* [LinkedIn](https://www.linkedin.com/in/massihforootan/)

## Portfolio Projects  

### 1. Student Life Data Integration Dashboard  
**Tools**: Python, Power BI, Semantic models 

Developed an automated data pipeline integrating data from multiple university departments for operational insights. Designed dashboards to track student engagement, program outcomes, and operational KPIs.

- Automated data cleaning and integration process from disparate systems.
- Created interactive dashboards reducing reporting time for stakeholders.
- Increased data visibility across Student Life Effectiveness initiatives.

**Key Features**:
- Live data connections  
- Multi-stakeholder reporting views  
- Role-based access dashboards  

---

### 2. Cancer Genomics Data QC Automation  
**Tools**: Python, Linux, REST-API  

Automated large-scale, unstructured genomic data quality control at the University of Chicago's Center for Translational Data Science.

- Cut data QC processing time by ~50%.
- Developed reusable Python scripts for error detection, data migration, and cleaning.
- Supported users via help desk, script writing, and documentation.

**Key Features**:
- Automated QC pipelines  
- API-based data retrieval & validation  
- End-user support & documentation  

---

### 3. Tennessee Integrated Traffic Analysis Dashboards  
**Tools**: Tableau, SQL  

Built 4 interactive dashboards replacing 100+ pages of monthly reports, driving operational insights for state traffic analysis:

#### Highlight: [Fatal and Serious Injury Crashes](https://www.tn.gov/safety/stats/dashboards/fatalseriousinjurycrashes.html)

This dashboard comprises near-real-time interactive information on fatal and serious injury collisions on Tennessee roadways for the current and previous years.

The dashboard enables a nuanced analysis of fatal and serious crashes through interactive filters and graphs, powered by a SQL database and Tableau. Users can analyze trends and patterns by location, road conditions, time of day, victim demographics, and other parameters. The dashboard provides actionable insights to inform traffic safety policies, enforcement initiatives, infrastructure improvements, public education campaigns, and other countermeasures aimed at reducing crash-related deaths and injuries on Tennessee roads.

The dashboard was presented at the 2019 LifeSavers Conference, an annual conference on injury prevention and traffic safety organized by the National Safety Council. ([Link](https://lifesaversconference.org/wp-content/uploads/2019/03/Forootan-ESP-06-a.pdf))

- Designed KPIs and interactive reports for law enforcement and policy teams.
- Earned a Pay-for-Performance (P4P) award for operational efficiency.

**Key Features**:
- Dynamic filtering & drill-downs  
- Multi-year traffic and incident trend analysis  

---

### 4. Data Quality Testing in Agribusiness Software  
**Tools**: Excel VBA  

Performed QA analysis at EFC Systems Inc, identifying algorithmic inconsistencies across platforms.

- Designed test scenarios mimicking real-world use.
- Discovered critical discrepancies between iOS and Windows software versions.

**Key Features**:
- Automated data checks  
- Cross-platform behavior validation  

---

### 5. Research Data Analysis and Experimental Design Projects  
**Tools**: Excel, VBA, SAS  

Oversaw 9 research projects supervising statistical analysis and experimental design.

- Taught statistics and experimental methodology.
- Developed low-cost protocols to optimize student research budgets.

**Key Features**:
- Multivariate analysis  
- Experimental designs  

**Publications**: [ORCID profile](https://orcid.org/0000-0002-8233-0630)

### 6. Miscellaneous Projects:
### 6.1. Land Use Change in Tennessee - Nashville  Software School
**Tools**: Python (SciKit), R (ShinyApp)  

* The objective of this project was to elucidate patterns and hypothetical rationales for land use changes in Tennessee. It comprised two sub-projects:

    - [Phase I](https://github.com/mforootan/NSS_MidStone_TN_Land_Use) (powered by R & Shiny App): Visualizing land use alterations utilizing an interactive dashboard.

    - [Phase II](https://github.com/mforootan/NSS_Capstone_TN_land_use) (powered by Python & SciKit): Identifying determinants contributing to land use changes via dimension reduction techniques.

* A slideshow reviewing the methodology and results is included with the phase II set of files.

* The visualizations depicted various correlations between land availability and valuation across counties. Analyses suggested the predominant criteria influencing land value are proximity to major metropolitan areas and agricultural profitability.

* In summary, this project leveraged statistical programming languages and multivariate analysis to glean scientific insights into drivers of land use trends. The interactive dashboards and dimension reduction models provide data-driven understanding of how exurban dynamics shape land use patterns over time. Further research could relate the identified factors to policies on zoning, land conservation, transportation infrastructure, and urban development.

**Key Features**:
- Principal Component Analysis
- k-Nearest Neighbors  

### 6.2. [Improve College-Going and College-Readiness](https://github.com/mforootan/TN_DoE_College-Readiness) - Division of Research and Evaluation, TN Dept of Education
**Tools**: Python

* This repository comprises the data and computer code (in Python, in a Jupyter notebook) for an assessment project with the objective of identifying measures that can evaluate high school graduates’ preparedness for collegiate studies.

* Researchers have a great interest in enhancing college readiness for all youth and increasing the rate of postsecondary enrollment among graduates. The community is equally passionate about improving its students’ postsecondary success. For many of these students, college is not perceived as a viable option. Researchers posit that the district can improve college readiness among all district students by first identifying high schools to serve as “models of excellence” and then learning from these exemplars about best practices for producing “college-ready” students who enroll and persist in postsecondary education. Therefore, the available data was examined to recommend a model school.

* Utilizing principal component analysis (aka dimension reduction), viable criteria were selected, upon which model schools were chosen. It appears that the most distinguishing factors to utilize for scoring are:

    * The standardized grades (mathematics, SAT assessments)
    * Enrolling for college. The two metrics that seem most relevant are: Enrolling for a 4-year degree (i.e., a strong education), and enrolling shortly after high school (an index of passion for education).

* However, regarding data quality, there were both missing values and duplicated entries in the dataset. The strategy to manage the data was deliberated and implemented. Further research avenues (disaggregating the data within sub-regions) were suggested.

**Key Features**:
- Principal Component Analysis

### 6.3. [Logistic Regression](https://github.com/mforootan/NSS_Stat_LogReg) - Nashville Software School
**Tools**: Python

* This Jupyter notebook was developed to exhibit the concept of logistic regression, and how to implement this technique in Python. The code generates a logistic regression model, prints the model summary, exports and prints the coefficients, calculates predicted probabilities, and visualizes the logistic regression model along with the original data. The logistic regression model is applied to a binary response variable based on an explanatory variable. The visualization helps to understand how well the logistic regression model fits the data.

---

## Technical Skills  

**Languages**: Python, R, SQL, GraphQL, DAX, VBA  
**Tools**: Power BI, Tableau, SAS  
**Techniques**: Experimental designs, Multivariate & Regressional Analysis, A/B testing  

---
##### ~ / M y _ L i f e / i n _ Z e r o e s _ a n d _ O n e $

###### I stepped into the programming world in my teenage years with a [CASIO FX-702P](https://www.google.com/search?q=casio+fx+702p) calculator. Equipped with a collection of built-in math functions and a BASIC programming language environment, it was a suitable tool for those who were interested in automating simple to semi-complex math problems. When 8-bit home computers became popular (for dual use of gaming and programming), my pick was a [ZX-Spectrum +2](https://www.google.com/search?q=zx+spectrum+%2B2), pretty much for the same trend of programming interests. 

###### During my undergraduate years, as 16 & 32-bit PCs became widespread, I started QBASIC and shortly afterward, [QuickBASIC](https://www.google.com/search?q=quickbasic). With genetics and statistics dominating my routine studies, digging into statistical programming turned into my top interest ([selected scripts](https://github.com/mforootan/QuickBASIC)). The big leap was starting to code for experimental designs and ANOVA, but before making significant progress, I learned about [MSTAT-C](https://www.google.com/search?q=mstatc), which sidetracked my interest in coding.

###### Meanwhile, Windows 3.x had already gotten into the market, and it was evident that the MS-DOS-based programming era was almost over. It didn’t take me long to find that Microsoft has released [Visual Basic](https://www.google.com/search?q=visual+basic+3), so I grabbed it and started migrating my old QB codes to the new platform to get my feet wet. With the upgrade to Windows 9x generation, I felt my programming skills had become obsolete, thus gradually abandoned programming and focused on mastering spreadsheets (started with Lotus 1-2-3 but quickly switched to [Excel](https://www.google.com/search?q=excel+4.0)) for data wrangling and exploration.

###### By starting postgraduate studies after a long gap, I was urged to use statistical tools again. The need for coding skills was raised again with Windows 98 in its glorious days and Windows 2000 and XP coming up, and statistical packages (Minitab, SPSS, and [SAS](https://support.sas.com/documentation/onlinedoc/91pdf/)) have become more user-friendly than ever, and learning resources were pretty well populated and accessible. Yet I could hardly be happier when found out that Visual Basic has become the kernel of macro language in Microsoft Office, specifically Excel. The [VBA](https://www.google.com/search?q=vba) scripting and SAS package thus became my main tools for programming and statistical analysis for nearly 10 years ([selected ANOVA templates](https://github.com/mforootan/SAS_ODS)).

###### After relocating to the United States, I initially joined the IT-agribusiness sector, but after a while realized that my knowledge in statistics and coding was perilously out of date. This founded an incentive to join a data science boot camp and add [Python](https://www.python.org/) and [R](https://www.r-project.org/) languages to my set of skills; followed by [SQL](https://www.w3schools.com/sql/) and [Tableau](https://www.tableau.com/) for business intelligence purposes after rejoining the job market. A data analysis career journey in road safety, cancer genomics, and now education industry has prompted exploring business management and data wrangling tools such as [REST-API](https://www.google.com/search?q=rest-api), [GraphQL](https://graphql.org/), [PowerBI](https://powerbi.microsoft.com/), and [Smartsheet](https://smartsheet.com/), and I'm looking forward to reconciling my cross-domain experience to develop solutions for seamless and robust data streaming.

##### . . / F u n _ F @ c t $
###### - My first email, registered in 1999, was an MS-DOS-based powered by [Pegasus Mail](http://www.pmail.com/overviews/ovw_pmail.htm). 
###### - I was among the first round of [Blogger](http://massihforootan.blogspot.com/) users who were invited by Google to register for Gmail in 2004 ([Story](https://www.theguardian.com/technology/blog/2004/apr/21/bloggerusersg)). Despite a wide range of usernames being available (including both my first and last name individually), I preferred to coin the very fourteen-character username I already had with Yahoo since 1999 (registered a few days after the Pegasus one) as my online identity. 
###### - Since then, I have owned email addresses with .net, .com, .ac.uk, .ac.ir, .gov, .org, and .edu domains; in that order.
###### - I am an Inbox Zero.

 

